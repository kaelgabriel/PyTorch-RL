{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append('..')\n",
    "from larocs_sim.envs.drone_env import DroneEnv\n",
    "\n",
    "from itertools import count\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_samples(deterministic=False):\n",
    "    #torch.randn(pid)\n",
    "    num_steps = 0\n",
    "\n",
    "    expert_traj = []\n",
    "        \n",
    "\n",
    "    for i_episode in count():\n",
    "\n",
    "        print('count = ',i_episode)\n",
    "        if i_episode % 5 == 0:\n",
    "            env.restart=True\n",
    "            env.reset()\n",
    "            env.restart=False\n",
    "        \n",
    "        state = env.reset()\n",
    "        state = running_state(state)\n",
    "        reward_episode = 0\n",
    "        \n",
    "        last_traj = []\n",
    "        for t in range(250):\n",
    "            if len(expert_traj) >= 2000/250:\n",
    "            \n",
    "                return expert_traj\n",
    "            \n",
    "            state_var = tensor(state).unsqueeze(0).to(dtype)\n",
    "            with torch.no_grad():\n",
    "                action = policy_net(state_var)[0][0].numpy() ## Chose mean action\n",
    "\n",
    "            action = int(action) if is_disc_action else action.astype(np.float64)\n",
    "\n",
    "            next_state, reward, done, _ = env.step(np.clip(action*100,a_min=-100, a_max=100))\n",
    "            next_state = running_state(next_state)\n",
    "\n",
    "            last_traj.append(np.hstack([state, action]))\n",
    "            \n",
    "            mask = 0 if done else 1\n",
    "            reward_episode += reward\n",
    "\n",
    "\n",
    "            if done:\n",
    "                \n",
    "                break\n",
    "        \n",
    "            state = next_state\n",
    "        if t == (250-1):\n",
    "            expert_traj.append(last_traj)\n",
    "    \n",
    "        print(\"Episode Reward = {0:.2f}\".format(reward_episode))\n",
    "        num_steps += (t + 1)\n",
    "\n",
    "\n",
    "    return expert_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scene =  /home/gabriel/repos/my_larocs_sim/larocs_sim/envs/../../scenes/ardrone_modeled_headless.ttt\n",
      "headless =  True\n",
      "initial_position =  [0.0, 0.0, 1.7000000476837158]\n",
      "initial_orientation =  [-0.0, 0.0, -0.0]\n",
      "Initial Position of the Target =  [0.0, 8.419156038996789e-09, 1.7000000476837158]\n",
      "creating discretized list\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float64\n",
    "torch.set_default_dtype(dtype)\n",
    "env_reset_mode='Discretized_Uniform'\n",
    "seed = 42\n",
    "try:\n",
    "    env.shutdown()\n",
    "except:\n",
    "    pass\n",
    "env = DroneEnv(random=env_reset_mode,seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "is_disc_action = len(env.action_space.shape) == 0\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "policy_net, _, running_state = pickle.load(open('../assets/learned_models/Teste_ppo_499.p', \"rb\"))\n",
    "running_state.fix = True\n",
    "expert_traj = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count =  0\n",
      "Episode Reward = 911.75\n",
      "count =  1\n",
      "Episode Reward = 908.77\n",
      "count =  2\n",
      "Episode Reward = 931.34\n",
      "count =  3\n",
      "Episode Reward = 938.17\n",
      "count =  4\n",
      "Episode Reward = 5.49\n",
      "count =  5\n",
      "Episode Reward = 900.02\n",
      "count =  6\n",
      "Episode Reward = 29.18\n",
      "count =  7\n",
      "Episode Reward = 931.63\n",
      "count =  8\n",
      "Episode Reward = 911.46\n",
      "count =  9\n",
      "Episode Reward = 932.99\n",
      "count =  10\n"
     ]
    }
   ],
   "source": [
    "deterministic = True\n",
    "expert_traj = collect_samples(deterministic = deterministic)\n",
    "expert_traj = np.concatenate(expert_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--env-name G] [--file G] [--seed N]\n",
      "                             [--max_expert_state_num N] [--H N]\n",
      "                             [--env_reset_mode ENV_RESET_MODE]\n",
      "                             [--deterministic]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/gabriel/.local/share/jupyter/runtime/kernel-a5328c4c-b757-4ba5-90cf-846190d8a981.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/anaconda3/envs/pyrep_raw/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3327: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pickle.dump((expert_traj, running_state), open(os.path.join(assets_dir(), 'expert_traj/{}_expert_traj_itrs_{}.p'.format(\\\n",
    "                    args.env_name, args.max_expert_state_num)), 'wb'))\n",
    "\n",
    "env.shutdown()                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('pyrep_raw': conda)",
   "language": "python",
   "name": "python37564bitpyreprawconda5bc433778dee457a96e41a43a28ab451"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
